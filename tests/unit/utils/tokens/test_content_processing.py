"""Unit tests for content processing modules."""

import pytest
import sys
import os

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../../../src"))

from src.utils.tokens.content_processor import ContentProcessor


class TestContentProcessor:
    """Test cases for ContentProcessor."""

    def setup_method(self):
        """Test setup."""
        self.processor = ContentProcessor()

    def test_content_processor_instantiation(self):
        """Test ContentProcessor instantiation."""
        assert self.processor is not None
        assert isinstance(self.processor, ContentProcessor)

    def test_token_estimation_basic(self):
        """Test basic token estimation."""
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºŽéªŒè¯tokenä¼°ç®—åŠŸèƒ½ã€‚"
        estimated_tokens = self.processor.estimate_tokens(test_text)

        assert isinstance(estimated_tokens, int)
        assert estimated_tokens > 0

    def test_token_estimation_empty_text(self):
        """Test token estimation with empty text."""
        empty_text = ""
        estimated_tokens = self.processor.estimate_tokens(empty_text)

        assert isinstance(estimated_tokens, int)
        assert estimated_tokens >= 0

    def test_token_estimation_various_lengths(self):
        """Test token estimation with various text lengths."""
        test_cases = [
            "çŸ­",  # Single character
            "çŸ­æ–‡æœ¬",  # Short text
            "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«å¤šä¸ªè¯æ±‡å’Œæ ‡ç‚¹ç¬¦å·ã€‚",  # Medium text
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ï¼Œ" * 20,  # Long text
        ]

        previous_tokens = 0
        for text in test_cases:
            estimated_tokens = self.processor.estimate_tokens(text)
            assert isinstance(estimated_tokens, int)
            assert estimated_tokens >= 0
            # Generally, longer text should have more tokens
            if len(text) > 0:
                assert estimated_tokens >= previous_tokens or estimated_tokens > 0
            previous_tokens = estimated_tokens

    def test_token_estimation_different_languages(self):
        """Test token estimation with different languages."""
        test_cases = [
            "Hello, this is English text.",
            "è¿™æ˜¯ä¸­æ–‡æ–‡æœ¬ã€‚",
            "ã“ã‚Œã¯æ—¥æœ¬èªžã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚",
            "Ð­Ñ‚Ð¾ Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ñ‚ÐµÐºÑÑ‚.",
            "Ceci est un texte franÃ§ais.",
        ]

        for text in test_cases:
            estimated_tokens = self.processor.estimate_tokens(text)
            assert isinstance(estimated_tokens, int)
            assert estimated_tokens > 0

    def test_content_sanitization_basic(self):
        """Test basic content sanitization."""
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºŽéªŒè¯å†…å®¹æ¸…ç†åŠŸèƒ½ã€‚"
        sanitized_content = self.processor.sanitize_content(test_text)

        assert isinstance(sanitized_content, str)
        assert len(sanitized_content) > 0

    def test_content_sanitization_empty_text(self):
        """Test content sanitization with empty text."""
        empty_text = ""
        sanitized_content = self.processor.sanitize_content(empty_text)

        assert isinstance(sanitized_content, str)
        assert len(sanitized_content) == 0

    def test_content_sanitization_special_characters(self):
        """Test content sanitization with special characters."""
        test_cases = [
            "Text with\nnewlines\nand\ttabs",
            "Text with <html> tags </html>",
            "Text with @#$%^&*() symbols",
            "Text with 'quotes' and \"double quotes\"",
            "Text with unicode: ðŸš€ ðŸŽ‰ âœ¨",
        ]

        for text in test_cases:
            sanitized_content = self.processor.sanitize_content(text)
            assert isinstance(sanitized_content, str)
            # Sanitized content should not be longer than original
            assert len(sanitized_content) <= len(text) + 10  # Allow some flexibility

    def test_content_sanitization_preserves_meaning(self):
        """Test that content sanitization preserves meaning."""
        test_text = "è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯ã€‚"
        sanitized_content = self.processor.sanitize_content(test_text)

        # Should preserve core content
        assert "æµ‹è¯•" in sanitized_content or "test" in sanitized_content.lower()
        assert len(sanitized_content) > 0

    def test_content_sanitization_consistency(self):
        """Test content sanitization consistency."""
        test_text = "ä¸€è‡´æ€§æµ‹è¯•æ–‡æœ¬"

        sanitized1 = self.processor.sanitize_content(test_text)
        sanitized2 = self.processor.sanitize_content(test_text)

        # Should produce consistent results
        assert sanitized1 == sanitized2

    def test_token_estimation_and_sanitization_integration(self):
        """Test integration between token estimation and content sanitization."""
        test_text = "è¿™æ˜¯ä¸€ä¸ªé›†æˆæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºŽéªŒè¯tokenä¼°ç®—å’Œå†…å®¹æ¸…ç†çš„ååŒå·¥ä½œã€‚"

        # Get original token count
        original_tokens = self.processor.estimate_tokens(test_text)

        # Sanitize content
        sanitized_content = self.processor.sanitize_content(test_text)

        # Get sanitized token count
        sanitized_tokens = self.processor.estimate_tokens(sanitized_content)

        # Both should be positive
        assert original_tokens > 0
        assert sanitized_tokens > 0

        # Sanitized tokens should be reasonable compared to original
        assert sanitized_tokens <= original_tokens * 1.5  # Allow some flexibility

    def test_multiple_processor_instances(self):
        """Test creating multiple processor instances."""
        processor1 = ContentProcessor()
        processor2 = ContentProcessor()

        assert processor1 is not processor2
        assert isinstance(processor1, ContentProcessor)
        assert isinstance(processor2, ContentProcessor)

        # Both should work independently
        test_text = "æµ‹è¯•æ–‡æœ¬"
        tokens1 = processor1.estimate_tokens(test_text)
        tokens2 = processor2.estimate_tokens(test_text)

        # Should produce same results
        assert tokens1 == tokens2

    def test_processor_error_handling(self):
        """Test processor error handling."""
        # Test with None input
        try:
            tokens = self.processor.estimate_tokens(None)
            assert isinstance(tokens, int) and tokens >= 0
        except (TypeError, AttributeError):
            # Exception is acceptable for None input
            pass

        try:
            sanitized = self.processor.sanitize_content(None)
            assert isinstance(sanitized, str)
        except (TypeError, AttributeError):
            # Exception is acceptable for None input
            pass

    @pytest.mark.parametrize(
        "test_input,expected_type",
        [
            ("Normal text", int),
            ("ä¸­æ–‡æ–‡æœ¬", int),
            ("Mixed ä¸­è‹±æ–‡ text", int),
            ("", int),
            ("Very long text " * 100, int),
        ],
    )
    def test_token_estimation_parametrized(self, test_input, expected_type):
        """Test token estimation with parametrized inputs."""
        result = self.processor.estimate_tokens(test_input)
        assert isinstance(result, expected_type)
        assert result >= 0

    @pytest.mark.parametrize(
        "test_input,expected_type",
        [
            ("Normal text", str),
            ("ä¸­æ–‡æ–‡æœ¬", str),
            ("Text with\nspecial\tchars", str),
            ("", str),
            ("HTML <tag>content</tag>", str),
        ],
    )
    def test_content_sanitization_parametrized(self, test_input, expected_type):
        """Test content sanitization with parametrized inputs."""
        result = self.processor.sanitize_content(test_input)
        assert isinstance(result, expected_type)

    def test_processor_performance_basic(self):
        """Test basic performance characteristics."""
        # Test with reasonably large text
        large_text = "è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½æµ‹è¯•æ–‡æœ¬ã€‚" * 1000

        # Should complete without hanging
        tokens = self.processor.estimate_tokens(large_text)
        sanitized = self.processor.sanitize_content(large_text)

        assert isinstance(tokens, int)
        assert isinstance(sanitized, str)
        assert tokens > 0
        assert len(sanitized) > 0
