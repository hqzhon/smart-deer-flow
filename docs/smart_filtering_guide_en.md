# Smart Filtering Feature User Guide

## Overview

The smart filtering feature is an important characteristic of the DeerFlow system, designed to solve the token limit issues caused by excessive information returned from web searches. This feature uses LLM models to intelligently filter and merge search results, retaining only the most relevant information to user queries.

## Feature Highlights

### ðŸŽ¯ Intelligent Relevance Filtering
- Uses LLM models to analyze the relevance between search results and queries
- Automatically filters irrelevant or low-quality content
- Retains the most valuable information fragments

### ðŸ“Š Batch Processing Mechanism
- Supports batch processing of large volumes of search results
- Avoids token limits in single processing operations
- Improves processing efficiency and stability

### ðŸ”„ Secondary Filtering Optimization
- Automatically performs secondary filtering when initial filtering results are still too long
- Further refines content to ensure compliance with token limits
- Maintains information integrity and accuracy

### âš¡ Smart Fallback Mechanism
- Automatically falls back to traditional processing methods when smart filtering fails
- Ensures system stability and reliability
- Provides multi-level fault tolerance protection

## Configuration Instructions

### 1. Configuration File Settings

Add the following configuration to the `conf.yaml` file:

```yaml
CONTENT_PROCESSING:
  enable_smart_chunking: true          # Enable smart chunking
  enable_content_summarization: true   # Enable content summarization
  enable_smart_filtering: true         # Enable smart filtering (new)
  chunk_strategy: "auto"               # Chunking strategy
  summary_type: "comprehensive"        # Summary type
```

### 2. Configuration Parameter Description

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `enable_smart_filtering` | boolean | `true` | Whether to enable LLM smart filtering feature |
| `enable_smart_chunking` | boolean | `true` | Whether to enable smart chunking (fallback option) |
| `enable_content_summarization` | boolean | `true` | Whether to enable content summarization (fallback option) |

## Workflow

### 1. Search Results Acquisition
```
User Query â†’ Search Engine â†’ Raw Search Results
```

### 2. Smart Filtering Processing
```
Raw Results â†’ Token Limit Check â†’ Smart Filtering â†’ Filtered Results
```

### 3. Fallback Mechanism
```
Filtering Failed â†’ Smart Summary â†’ Smart Chunking â†’ Final Results
```

## Core Components

### SearchResultFilter Class

Location: `src/utils/search_result_filter.py`

Main methods:
- `filter_search_results()`: Main filtering method
- `_create_filter_prompt()`: Create filtering prompt
- `_parse_filter_response()`: Parse filtering response
- `_format_filtered_results()`: Format filtered results

### ContentProcessor Integration

Location: `src/utils/content_processor.py`

The enhanced `process_search_results()` method now supports:
- `query` parameter: User query content
- `enable_smart_filtering` parameter: Whether to enable smart filtering

## Usage Examples

### 1. Basic Usage

```python
from src.utils.common.search_result_filter import SearchResultFilter
from src.utils.tokens.content_processor import ContentProcessor

# Create content processor and filter instances
processor = ContentProcessor()
filter_instance = SearchResultFilter(processor)

# Execute filtering (no longer requires LLM parameter)
filtered_results = filter_instance.filter_search_results(
    query="Python machine learning tutorial",
    search_results=search_results,
    model_name="deepseek-chat",
    max_results=3
)
```

### 2. Integration with Content Processor

```python
from src.utils.tokens.content_processor import ContentProcessor
from src.utils.common.search_result_filter import SearchResultFilter

# Create processor and filter instances
processor = ContentProcessor()
filter_instance = SearchResultFilter(processor)

# Process search results (using new lightweight filtering)
filtered_results = filter_instance.filter_search_results(
    query="Deep learning framework comparison",
    search_results=search_results,
    model_name="deepseek-chat",
    max_results=5
)
```

## Performance Optimization

### 1. Batch Size
- Default batch size: 5 results
- Can be adjusted based on model token limits
- Balance processing efficiency and quality

### 2. Relevance Threshold
- Default relevance threshold: 7.0 (out of 10)
- Can be adjusted based on application scenarios
- Higher threshold = stricter filtering

### 3. Secondary Filtering Trigger
- Triggered when filtering results exceed 80% of token limit
- Further compresses content length
- Maintains information quality

## Monitoring and Logging

### Log Levels
- `INFO`: Key information about filtering process
- `DEBUG`: Detailed filtering steps
- `WARNING`: Filtering failures or exceptions
- `ERROR`: Serious errors and exceptions

### Key Metrics
- Changes in result count before and after filtering
- Content length compression ratio
- Filtering success rate
- Processing time

## Troubleshooting

### Common Issues

1. **Empty filtering results**
   - Check relevance threshold settings
   - Confirm matching degree between query and search results
   - Review LLM response logs

2. **Frequent filtering failures**
   - Check LLM model configuration
   - Ensure stable network connection
   - Review error log details

3. **Performance issues**
   - Adjust batch size
   - Optimize relevance threshold
   - Consider using faster LLM models

### Debug Mode

Enable detailed logging:
```python
import logging
logging.getLogger('src.utils.search_result_filter').setLevel(logging.DEBUG)
```

## Test Validation

Run test scripts to validate functionality:

```bash
python test_smart_filtering.py
```

Test content includes:
- Basic filtering functionality
- Content processor integration
- Error handling mechanisms
- Performance benchmarks

## Best Practices

### 1. Configuration Recommendations
- Enable all smart processing features in production environment
- Adjust relevant parameters based on actual token limits
- Regularly monitor filtering effectiveness and performance

### 2. Query Optimization
- Use specific, clear query terms
- Avoid overly broad queries
- Consider language and domain characteristics of queries

### 3. Result Validation
- Regularly check quality of filtered results
- Collect user feedback for optimization
- Monitor relevance score distribution

## Future Improvements

### Planned Features
- Support for custom filtering strategies
- Multi-language filtering support
- Filtering result caching mechanism
- Filtering effectiveness evaluation tools

### Performance Optimization
- Parallel processing of multiple batches
- Smart batch size adjustment
- Caching of common query filtering results

---

## Technical Support

If you have questions or suggestions, please:
1. Check log files for detailed error information
2. Run test scripts to verify functionality status
3. Check configuration file settings for correctness
4. Confirm LLM model availability and configuration